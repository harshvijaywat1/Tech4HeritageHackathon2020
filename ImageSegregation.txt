import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.linalg.Vectors

val image_df = spark.read.format("image").load("/home/harsh/Desktop/unnamed.jpg")
val img_array = image_df.select(col("image.data"))
      .rdd.flatMap(f => f.getAs[Array[Byte]]("data"))

    val img_data = image_df.select(col("image.*"))
      .rdd.map(row => (
        row.getAs[Int]("height"),
        row.getAs[Int]("width"),
        row.getAs[Int]("nChannels"),
        row.getAs[Array[Byte]]("data")))
      .collect()(0)

    val height = img_data._1
    val width = img_data._2
    val nChannels = img_data._3
    
    var offSet = spark.sparkContext.longAccumulator("offSetAcc")
    var x = spark.sparkContext.longAccumulator("xAcc")
    var y = spark.sparkContext.longAccumulator("yAcc")
    x.add(1)
    y.add(1)

import spark.implicits._
var final_image_df = img_array.zipWithIndex().map { f =>

      if (offSet.value == 0) {
        offSet.add(1)
        if (f._2 != 0)
          x.add(1)
      } else if (offSet.value == 1) {
        //g
        offSet.add(1)
      } else if (offSet.value == 2) {
        //r
        offSet.reset()
      }
      if (x.value == (width)) {
        x.reset()
        y.add(1)
      }

      (f._1 & 0xFF, x.value, y.value)
    }.toDF.withColumnRenamed("_1", "b")
      .withColumnRenamed("_2", "w").withColumnRenamed("_3", "h")

final_image_df = final_image_df.groupBy(col("w"), col("h"))
      .agg(collect_list(col("color")).as("color"))
      .orderBy(col("w"), col("h"))
      .rdd
      .map { f =>
        val a = f(2).asInstanceOf[WrappedArray[Int]]
        (f(0).toString.toInt, f(1).toString.toInt, a(0), a(1), a(2))
      }
      .toDF
      .withColumnRenamed("_1", "w")
      .withColumnRenamed("_2", "h")
      .withColumnRenamed("_3", "b")
      .withColumnRenamed("_4", "g")
      .withColumnRenamed("_5", "r")


val features_col = Array("r","g","b")
    val vector_assembler = new VectorAssembler()
    .setInputCols(features_col)
    .setOutputCol("features")
 
    val features_df = vector_assembler.transform(final_image_df).select("features")
  
    val cost = new Array[Double](21)

    for(k<-2 to 20){
      val kmeans = new KMeans().setK(k).setSeed(1L).setFeaturesCol("features")
      val model =  kmeans.fit(features_df.sample(false,0.1))
      cost(k) = model.computeCost(features_df)
    }

    var plotSeq = 
        cost.zipWithIndex.map{case (l,i)=> collection.mutable.Map("clusters" -> i, "cost" -> l)}
    .map(p => p.retain((k,v) => v != 0)).map(f => Map(f.toSeq:_*))
  
import java.io._      
    val writer = new PrintWriter(new File("/home/harsh/Desktop/unnamed1.jpg"))

    val plot = Vegas("Kmeans Cost").withData(plotSeq).
      encodeX("clusters", Nom).
      encodeY("cost", Quant).
      mark(Line)

     writer.write(plot.html.pageHTML("plot"))
     writer.close()


